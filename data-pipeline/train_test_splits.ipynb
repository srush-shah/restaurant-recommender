{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af963e6c-a799-4fca-ba22-d017f7ade131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, count, row_number, lit, when, rand, floor\n",
    "from pyspark.sql.window import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e262c-0fa3-41e5-9376-12c7bf7d1bd6",
   "metadata": {},
   "source": [
    "### Initialize Spark Session and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb4833e-acb8-4ada-9222-7a9b99893e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Reading CSVs with inferSchema=True ---\n",
      "Reading CSV file: data/als/training_data.csv\n",
      "Successfully read 928878 records from data/als/training_data.csv.\n",
      "Schema after standardizing core ALS columns for data/als/training_data.csv:\n",
      "Reading CSV file: data/als/production_data.csv\n",
      "Successfully read 4164772 records from data/als/production_data.csv.\n",
      "Schema after standardizing core ALS columns for data/als/production_data.csv:\n",
      "Reading CSV file: data/als/validation_data.csv\n",
      "Successfully read 32637 records from data/als/validation_data.csv.\n",
      "Schema after standardizing core ALS columns for data/als/validation_data.csv:\n",
      "\n",
      "--- Combined DataFrame ---\n",
      "Total records after union: 5126287\n",
      "\n",
      "--- Performing deduplication based on ['user_id', 'business_id', 'stars'] ---\n",
      "Total records after deduplication: 5027330\n",
      "\n",
      "--- DataFrame ready for ID mapping ---\n",
      "Joining combined data with mapping tables and saving the mappings...\n",
      "\n",
      "--- Count of the new comprehensive als_input_df_loaded ---\n",
      "Total records in new als_input_df_loaded: 5027330\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Combine_All_Features_ALS_MLP\") \\\n",
    "    .config(\"spark.driver.memory\", \"128g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define paths to your CSV files\n",
    "file_path_train = \"data/als/training_data.csv\"\n",
    "file_path_prod = \"data/als/production_data.csv\"  \n",
    "file_path_val = \"data/als/validation_data.csv\"    \n",
    "\n",
    "csv_files_to_load = [file_path_train, file_path_prod, file_path_val]\n",
    "all_dataframes = []\n",
    "\n",
    "master_schema = spark.read.csv(csv_files_to_load[0], header=True, inferSchema=True).schema\n",
    "combined_parquet_path = \"data/combined_features_deduplicated.parquet\"\n",
    "\n",
    "print(\"--- Reading CSVs with inferSchema=True ---\")\n",
    "for file_path in csv_files_to_load:\n",
    "    print(f\"Reading CSV file: {file_path}\")\n",
    "    try:\n",
    "        df_temp = spark.read.csv(file_path, header=True, schema=master_schema)\n",
    "        print(f\"Successfully read {df_temp.count()} records from {file_path}.\")\n",
    "\n",
    "        critical_als_cols_to_select = [\n",
    "            col(\"user_id\").cast(\"string\").alias(\"user_id\"), # Ensure string for joining with mapping\n",
    "            col(\"business_id\").cast(\"string\").alias(\"business_id\"), # Ensure string for joining with mapping\n",
    "            col(\"stars\").cast(\"double\").alias(\"stars\") # Ensure numeric for rating\n",
    "        ]\n",
    "        \n",
    "        # Get list of other columns to carry over with their types\n",
    "        other_cols = [c for c in df_temp.columns if c.lower() not in [\"user_id\", \"business_id\", \"stars\"]]\n",
    "        select_exprs = critical_als_cols_to_select + [col(c) for c in other_cols]\n",
    "        \n",
    "        df_temp_standardized_core = df_temp.select(select_exprs)\n",
    "        \n",
    "        all_dataframes.append(df_temp_standardized_core)\n",
    "        print(f\"Schema after standardizing core ALS columns for {file_path}:\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        # Decide how to handle errors: skip the file, stop execution, etc.\n",
    "\n",
    "if not all_dataframes:\n",
    "    raise ValueError(\"No DataFrames were successfully processed. Please check file paths and CSV formats.\")\n",
    "\n",
    "# Combine all DataFrames using unionByName\n",
    "# This should now be more robust for the critical columns.\n",
    "combined_df = None\n",
    "if len(all_dataframes) > 0:\n",
    "    # Check if all dataframes to be unioned have the same column names (order doesn't matter for unionByName)\n",
    "    # This is a basic check; type compatibility for non-critical columns is still managed by Spark's unionByName.\n",
    "    base_columns = set(all_dataframes[0].columns)\n",
    "    for i, df_to_union in enumerate(all_dataframes[1:], 1):\n",
    "        if set(df_to_union.columns) != base_columns:\n",
    "            print(f\"Warning: DataFrame from file {csv_files_to_load[i]} has different columns than the first DataFrame.\")\n",
    "            print(f\"Base columns: {sorted(list(base_columns))}\")\n",
    "            print(f\"Columns in DF {i}: {sorted(list(set(df_to_union.columns)))}\")\n",
    "            print(\"Ensure 'unionByName' can resolve this or select a common subset of columns for all DataFrames if this is an issue.\")\n",
    "            # For safety, you might want to intersect columns here or handle error.\n",
    "            # common_cols = list(base_columns.intersection(set(df_to_union.columns)))\n",
    "            # all_dataframes[0] = all_dataframes[0].select(common_cols)\n",
    "            # all_dataframes[i] = df_to_union.select(common_cols)\n",
    "            # base_columns = set(common_cols)\n",
    "\n",
    "\n",
    "    combined_df = all_dataframes[0]\n",
    "    if len(all_dataframes) > 1:\n",
    "        for i in range(1, len(all_dataframes)):\n",
    "            # Using allowMissingColumns=True can be helpful if some non-critical columns\n",
    "            # only exist in some files, they will be filled with nulls where missing.\n",
    "            # However, ensure this is the desired behavior.\n",
    "            combined_df = combined_df.unionByName(all_dataframes[i], allowMissingColumns=True)\n",
    "    print(f\"\\n--- Combined DataFrame ---\")\n",
    "    print(f\"Total records after union: {combined_df.count()}\")\n",
    "else:\n",
    "    print(\"No data was loaded to combine.\")\n",
    "    spark.stop()\n",
    "    exit()\n",
    "\n",
    "unique_key_columns = [\"user_id\", \"business_id\", \"stars\"] # Add other columns that define a unique record\n",
    "# Check if these columns exist before trying to use them\n",
    "missing_keys = [key for key in unique_key_columns if key not in combined_df.columns]\n",
    "if missing_keys:\n",
    "    raise ValueError(f\"Missing one or more key columns for deduplication: {missing_keys}. Available columns: {combined_df.columns}\")\n",
    "\n",
    "print(f\"\\n--- Performing deduplication based on {unique_key_columns} ---\")\n",
    "distinct_combined_df = combined_df.dropDuplicates(unique_key_columns)\n",
    "distinct_combined_df.cache() # Cache the final clean dataset\n",
    "print(f\"Total records after deduplication: {distinct_combined_df.count()}\")\n",
    "# distinct_combined_df.show(3, truncate=False)\n",
    "combined_df.unpersist() # Unpersist the pre-deduplication version\n",
    "\n",
    "\n",
    "df_for_mapping = distinct_combined_df\n",
    "\n",
    "print(\"\\n--- DataFrame ready for ID mapping ---\")\n",
    "\n",
    "# --- Continue with your ID mapping and ALS input preparation ---\n",
    "user_id_mapping_loaded = combined_df.select(\"user_id\").distinct().withColumn(\"userCol\", monotonically_increasing_id())\n",
    "business_id_mapping_loaded = combined_df.select(\"business_id\").distinct().withColumn(\"itemCol\", monotonically_increasing_id())\n",
    "\n",
    "print(\"Joining combined data with mapping tables and saving the mappings...\")\n",
    "# The join keys \"user_id\" and \"business_id\" in df_for_mapping are already strings here.\n",
    "als_input_df_loaded = df_for_mapping.join(user_id_mapping_loaded, \"user_id\", \"inner\")\n",
    "als_input_df_loaded = als_input_df_loaded.join(business_id_mapping_loaded, \"business_id\", \"inner\")\n",
    "\n",
    "# Now select the final columns needed for ALS, ensuring correct types\n",
    "# userCol and itemCol come from your mapping tables (should be int)\n",
    "# stars (rating) was already cast to double\n",
    "# als_input_df_mapped = als_input_df_loaded.select(\n",
    "#     col(\"userCol\").cast(\"int\"),\n",
    "#     col(\"itemCol\").cast(\"int\"),\n",
    "#     col(\"stars\").alias(\"rating\") # stars is already double, aliasing to rating\n",
    "#     # If you need any of the other 300+ features for your ALS model (uncommon for standard ALS),\n",
    "#     # you would select and process them here. Standard ALS only uses user, item, rating.\n",
    "# )\n",
    "\n",
    "print(\"\\n--- Count of the new comprehensive als_input_df_loaded ---\")\n",
    "print(f\"Total records in new als_input_df_loaded: {als_input_df_loaded.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242817e0-8260-48ca-8826-d74950561d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('business_id', StringType(), True), StructField('user_id', StringType(), True), StructField('stars', DoubleType(), True), StructField('date', TimestampType(), True), StructField('city', StringType(), True), StructField('category_0', DoubleType(), True), StructField('category_1', DoubleType(), True), StructField('category_2', DoubleType(), True), StructField('category_3', DoubleType(), True), StructField('category_4', DoubleType(), True), StructField('category_5', DoubleType(), True), StructField('category_6', DoubleType(), True), StructField('category_7', DoubleType(), True), StructField('category_8', DoubleType(), True), StructField('category_9', DoubleType(), True), StructField('category_10', DoubleType(), True), StructField('category_11', DoubleType(), True), StructField('category_12', DoubleType(), True), StructField('category_13', DoubleType(), True), StructField('category_14', DoubleType(), True), StructField('category_15', DoubleType(), True), StructField('category_16', DoubleType(), True), StructField('category_17', DoubleType(), True), StructField('category_18', DoubleType(), True), StructField('category_19', DoubleType(), True), StructField('category_20', DoubleType(), True), StructField('category_21', DoubleType(), True), StructField('category_22', DoubleType(), True), StructField('category_23', DoubleType(), True), StructField('category_24', DoubleType(), True), StructField('category_25', DoubleType(), True), StructField('category_26', DoubleType(), True), StructField('category_27', DoubleType(), True), StructField('category_28', DoubleType(), True), StructField('category_29', DoubleType(), True), StructField('category_30', DoubleType(), True), StructField('category_31', DoubleType(), True), StructField('category_32', DoubleType(), True), StructField('category_33', DoubleType(), True), StructField('category_34', DoubleType(), True), StructField('category_35', DoubleType(), True), StructField('category_36', DoubleType(), True), StructField('category_37', DoubleType(), True), StructField('category_38', DoubleType(), True), StructField('category_39', DoubleType(), True), StructField('category_40', DoubleType(), True), StructField('category_41', DoubleType(), True), StructField('category_42', DoubleType(), True), StructField('category_43', DoubleType(), True), StructField('category_44', DoubleType(), True), StructField('category_45', DoubleType(), True), StructField('category_46', DoubleType(), True), StructField('category_47', DoubleType(), True), StructField('category_48', DoubleType(), True), StructField('category_49', DoubleType(), True), StructField('category_50', DoubleType(), True), StructField('category_51', DoubleType(), True), StructField('category_52', DoubleType(), True), StructField('category_53', DoubleType(), True), StructField('category_54', DoubleType(), True), StructField('category_55', DoubleType(), True), StructField('category_56', DoubleType(), True), StructField('category_57', DoubleType(), True), StructField('category_58', DoubleType(), True), StructField('category_59', DoubleType(), True), StructField('category_60', DoubleType(), True), StructField('category_61', DoubleType(), True), StructField('category_62', DoubleType(), True), StructField('category_63', DoubleType(), True), StructField('category_64', DoubleType(), True), StructField('category_65', DoubleType(), True), StructField('category_66', DoubleType(), True), StructField('category_67', DoubleType(), True), StructField('category_68', DoubleType(), True), StructField('category_69', DoubleType(), True), StructField('category_70', DoubleType(), True), StructField('category_71', DoubleType(), True), StructField('category_72', DoubleType(), True), StructField('category_73', DoubleType(), True), StructField('category_74', DoubleType(), True), StructField('category_75', DoubleType(), True), StructField('category_76', DoubleType(), True), StructField('category_77', DoubleType(), True), StructField('category_78', DoubleType(), True), StructField('category_79', DoubleType(), True), StructField('category_80', DoubleType(), True), StructField('category_81', DoubleType(), True), StructField('category_82', DoubleType(), True), StructField('category_83', DoubleType(), True), StructField('category_84', DoubleType(), True), StructField('category_85', DoubleType(), True), StructField('category_86', DoubleType(), True), StructField('category_87', DoubleType(), True), StructField('category_88', DoubleType(), True), StructField('category_89', DoubleType(), True), StructField('category_90', DoubleType(), True), StructField('category_91', DoubleType(), True), StructField('category_92', DoubleType(), True), StructField('category_93', DoubleType(), True), StructField('category_94', DoubleType(), True), StructField('category_95', DoubleType(), True), StructField('category_96', DoubleType(), True), StructField('category_97', DoubleType(), True), StructField('category_98', DoubleType(), True), StructField('category_99', DoubleType(), True), StructField('category_100', DoubleType(), True), StructField('category_101', DoubleType(), True), StructField('category_102', DoubleType(), True), StructField('category_103', DoubleType(), True), StructField('category_104', DoubleType(), True), StructField('category_105', DoubleType(), True), StructField('category_106', DoubleType(), True), StructField('category_107', DoubleType(), True), StructField('category_108', DoubleType(), True), StructField('category_109', DoubleType(), True), StructField('category_110', DoubleType(), True), StructField('category_111', DoubleType(), True), StructField('category_112', DoubleType(), True), StructField('category_113', DoubleType(), True), StructField('category_114', DoubleType(), True), StructField('category_115', DoubleType(), True), StructField('category_116', DoubleType(), True), StructField('category_117', DoubleType(), True), StructField('category_118', DoubleType(), True), StructField('category_119', DoubleType(), True), StructField('category_120', DoubleType(), True), StructField('category_121', DoubleType(), True), StructField('category_122', DoubleType(), True), StructField('category_123', DoubleType(), True), StructField('category_124', DoubleType(), True), StructField('category_125', DoubleType(), True), StructField('category_126', DoubleType(), True), StructField('category_127', DoubleType(), True), StructField('category_128', DoubleType(), True), StructField('category_129', DoubleType(), True), StructField('category_130', DoubleType(), True), StructField('category_131', DoubleType(), True), StructField('category_132', DoubleType(), True), StructField('category_133', DoubleType(), True), StructField('category_134', DoubleType(), True), StructField('category_135', DoubleType(), True), StructField('category_136', DoubleType(), True), StructField('category_137', DoubleType(), True), StructField('category_138', DoubleType(), True), StructField('category_139', DoubleType(), True), StructField('category_140', DoubleType(), True), StructField('category_141', DoubleType(), True), StructField('category_142', DoubleType(), True), StructField('category_143', DoubleType(), True), StructField('category_144', DoubleType(), True), StructField('category_145', DoubleType(), True), StructField('category_146', DoubleType(), True), StructField('category_147', DoubleType(), True), StructField('category_148', DoubleType(), True), StructField('category_149', DoubleType(), True), StructField('category_150', DoubleType(), True), StructField('category_151', DoubleType(), True), StructField('category_152', DoubleType(), True), StructField('category_153', DoubleType(), True), StructField('category_154', DoubleType(), True), StructField('category_155', DoubleType(), True), StructField('category_156', DoubleType(), True), StructField('category_157', DoubleType(), True), StructField('category_158', DoubleType(), True), StructField('category_159', DoubleType(), True), StructField('category_160', DoubleType(), True), StructField('category_161', DoubleType(), True), StructField('category_162', DoubleType(), True), StructField('category_163', DoubleType(), True), StructField('category_164', DoubleType(), True), StructField('category_165', DoubleType(), True), StructField('category_166', DoubleType(), True), StructField('category_167', DoubleType(), True), StructField('category_168', DoubleType(), True), StructField('category_169', DoubleType(), True), StructField('category_170', DoubleType(), True), StructField('category_171', DoubleType(), True), StructField('category_172', DoubleType(), True), StructField('category_173', DoubleType(), True), StructField('category_174', DoubleType(), True), StructField('category_175', DoubleType(), True), StructField('category_176', DoubleType(), True), StructField('category_177', DoubleType(), True), StructField('category_178', DoubleType(), True), StructField('category_179', DoubleType(), True), StructField('category_180', DoubleType(), True), StructField('category_181', DoubleType(), True), StructField('category_182', DoubleType(), True), StructField('category_183', DoubleType(), True), StructField('category_184', DoubleType(), True), StructField('category_185', DoubleType(), True), StructField('category_186', DoubleType(), True), StructField('category_187', DoubleType(), True), StructField('category_188', DoubleType(), True), StructField('category_189', DoubleType(), True), StructField('category_190', DoubleType(), True), StructField('category_191', DoubleType(), True), StructField('category_192', DoubleType(), True), StructField('category_193', DoubleType(), True), StructField('category_194', DoubleType(), True), StructField('category_195', DoubleType(), True), StructField('category_196', DoubleType(), True), StructField('category_197', DoubleType(), True), StructField('category_198', DoubleType(), True), StructField('category_199', DoubleType(), True), StructField('category_200', DoubleType(), True), StructField('category_201', DoubleType(), True), StructField('category_202', DoubleType(), True), StructField('category_203', DoubleType(), True), StructField('category_204', DoubleType(), True), StructField('category_205', DoubleType(), True), StructField('category_206', DoubleType(), True), StructField('category_207', DoubleType(), True), StructField('category_208', DoubleType(), True), StructField('category_209', DoubleType(), True), StructField('category_210', DoubleType(), True), StructField('category_211', DoubleType(), True), StructField('category_212', DoubleType(), True), StructField('category_213', DoubleType(), True), StructField('category_214', DoubleType(), True), StructField('category_215', DoubleType(), True), StructField('category_216', DoubleType(), True), StructField('category_217', DoubleType(), True), StructField('category_218', DoubleType(), True), StructField('category_219', DoubleType(), True), StructField('category_220', DoubleType(), True), StructField('category_221', DoubleType(), True), StructField('category_222', DoubleType(), True), StructField('category_223', DoubleType(), True), StructField('category_224', DoubleType(), True), StructField('category_225', DoubleType(), True), StructField('category_226', DoubleType(), True), StructField('category_227', DoubleType(), True), StructField('category_228', DoubleType(), True), StructField('category_229', DoubleType(), True), StructField('category_230', DoubleType(), True), StructField('category_231', DoubleType(), True), StructField('category_232', DoubleType(), True), StructField('category_233', DoubleType(), True), StructField('category_234', DoubleType(), True), StructField('category_235', DoubleType(), True), StructField('category_236', DoubleType(), True), StructField('category_237', DoubleType(), True), StructField('category_238', DoubleType(), True), StructField('category_239', DoubleType(), True), StructField('category_240', DoubleType(), True), StructField('category_241', DoubleType(), True), StructField('category_242', DoubleType(), True), StructField('category_243', DoubleType(), True), StructField('category_244', DoubleType(), True), StructField('category_245', DoubleType(), True), StructField('category_246', DoubleType(), True), StructField('category_247', DoubleType(), True), StructField('category_248', DoubleType(), True), StructField('category_249', DoubleType(), True), StructField('category_250', DoubleType(), True), StructField('category_251', DoubleType(), True), StructField('category_252', DoubleType(), True), StructField('category_253', DoubleType(), True), StructField('category_254', DoubleType(), True), StructField('category_255', DoubleType(), True), StructField('category_256', DoubleType(), True), StructField('category_257', DoubleType(), True), StructField('category_258', DoubleType(), True), StructField('category_259', DoubleType(), True), StructField('category_260', DoubleType(), True), StructField('category_261', DoubleType(), True), StructField('category_262', DoubleType(), True), StructField('category_263', DoubleType(), True), StructField('category_264', DoubleType(), True), StructField('category_265', DoubleType(), True), StructField('category_266', DoubleType(), True), StructField('category_267', DoubleType(), True), StructField('category_268', DoubleType(), True), StructField('category_269', DoubleType(), True), StructField('category_270', DoubleType(), True), StructField('category_271', DoubleType(), True), StructField('category_272', DoubleType(), True), StructField('category_273', DoubleType(), True), StructField('category_274', DoubleType(), True), StructField('category_275', DoubleType(), True), StructField('category_276', DoubleType(), True), StructField('category_277', DoubleType(), True), StructField('category_278', DoubleType(), True), StructField('category_279', DoubleType(), True), StructField('category_280', DoubleType(), True), StructField('category_281', DoubleType(), True), StructField('category_282', DoubleType(), True), StructField('category_283', DoubleType(), True), StructField('category_284', DoubleType(), True), StructField('category_285', DoubleType(), True), StructField('category_286', DoubleType(), True), StructField('category_287', DoubleType(), True), StructField('category_288', DoubleType(), True), StructField('category_289', DoubleType(), True), StructField('category_290', DoubleType(), True), StructField('category_291', DoubleType(), True), StructField('category_292', DoubleType(), True), StructField('category_293', DoubleType(), True), StructField('category_294', DoubleType(), True), StructField('category_295', DoubleType(), True), StructField('category_296', DoubleType(), True), StructField('category_297', DoubleType(), True), StructField('category_298', DoubleType(), True), StructField('category_299', DoubleType(), True), StructField('category_300', DoubleType(), True), StructField('category_301', DoubleType(), True), StructField('category_302', DoubleType(), True), StructField('category_303', DoubleType(), True), StructField('category_304', DoubleType(), True), StructField('category_305', DoubleType(), True), StructField('category_306', DoubleType(), True), StructField('category_307', DoubleType(), True), StructField('category_308', DoubleType(), True), StructField('category_309', DoubleType(), True), StructField('category_310', DoubleType(), True), StructField('category_311', DoubleType(), True), StructField('category_312', DoubleType(), True), StructField('category_313', DoubleType(), True), StructField('category_314', DoubleType(), True), StructField('category_315', DoubleType(), True), StructField('category_316', DoubleType(), True), StructField('category_317', DoubleType(), True), StructField('category_318', DoubleType(), True), StructField('category_319', DoubleType(), True), StructField('category_320', DoubleType(), True), StructField('category_321', DoubleType(), True), StructField('category_322', DoubleType(), True), StructField('category_323', DoubleType(), True), StructField('category_324', DoubleType(), True), StructField('category_325', DoubleType(), True), StructField('category_326', DoubleType(), True), StructField('category_327', DoubleType(), True), StructField('category_328', DoubleType(), True), StructField('category_329', DoubleType(), True), StructField('category_330', DoubleType(), True), StructField('category_331', DoubleType(), True), StructField('category_332', DoubleType(), True), StructField('category_333', DoubleType(), True), StructField('category_334', DoubleType(), True), StructField('category_335', DoubleType(), True), StructField('category_336', DoubleType(), True), StructField('category_337', DoubleType(), True), StructField('category_338', DoubleType(), True), StructField('category_339', DoubleType(), True), StructField('category_340', DoubleType(), True), StructField('category_341', DoubleType(), True), StructField('category_342', DoubleType(), True), StructField('category_343', DoubleType(), True), StructField('category_344', DoubleType(), True), StructField('category_345', DoubleType(), True), StructField('category_346', DoubleType(), True), StructField('category_347', DoubleType(), True), StructField('category_348', DoubleType(), True), StructField('category_349', DoubleType(), True), StructField('category_350', DoubleType(), True), StructField('category_351', DoubleType(), True), StructField('category_352', DoubleType(), True), StructField('category_353', DoubleType(), True), StructField('category_354', DoubleType(), True), StructField('category_355', DoubleType(), True), StructField('category_356', DoubleType(), True), StructField('category_357', DoubleType(), True), StructField('category_358', DoubleType(), True), StructField('category_359', DoubleType(), True), StructField('category_360', DoubleType(), True), StructField('category_361', DoubleType(), True), StructField('category_362', DoubleType(), True), StructField('category_363', DoubleType(), True), StructField('category_364', DoubleType(), True), StructField('category_365', DoubleType(), True), StructField('category_366', DoubleType(), True), StructField('category_367', DoubleType(), True), StructField('category_368', DoubleType(), True), StructField('category_369', DoubleType(), True), StructField('category_370', DoubleType(), True), StructField('category_371', DoubleType(), True), StructField('category_372', DoubleType(), True), StructField('category_373', DoubleType(), True), StructField('category_374', DoubleType(), True), StructField('category_375', DoubleType(), True), StructField('category_376', DoubleType(), True), StructField('category_377', DoubleType(), True), StructField('category_378', DoubleType(), True), StructField('category_379', DoubleType(), True), StructField('category_380', DoubleType(), True), StructField('category_381', DoubleType(), True), StructField('category_382', DoubleType(), True), StructField('category_383', DoubleType(), True), StructField('category_384', DoubleType(), True), StructField('category_385', DoubleType(), True), StructField('category_386', DoubleType(), True), StructField('category_387', DoubleType(), True), StructField('category_388', DoubleType(), True), StructField('category_389', DoubleType(), True), StructField('category_390', DoubleType(), True), StructField('category_391', DoubleType(), True), StructField('category_392', DoubleType(), True), StructField('category_393', DoubleType(), True), StructField('category_394', DoubleType(), True), StructField('category_395', DoubleType(), True), StructField('category_396', DoubleType(), True), StructField('category_397', DoubleType(), True), StructField('category_398', DoubleType(), True), StructField('category_399', DoubleType(), True), StructField('category_400', DoubleType(), True), StructField('category_401', DoubleType(), True), StructField('userCol', LongType(), False), StructField('itemCol', LongType(), False)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_input_df_loaded.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2696d6-3f34-452f-8d15-520ce1a31d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset record count: 5027330\n",
      "\n",
      "Starting K-core filtering...\n",
      "  K-core Iteration 1: Starting with 5027330 records.\n",
      "  K-core Iteration 1: Ended with 3665543 records.\n",
      "  K-core Iteration 2: Starting with 3665543 records.\n",
      "  K-core Iteration 2: Ended with 3665225 records.\n",
      "  K-core Iteration 3: Starting with 3665225 records.\n",
      "  K-core Iteration 3: Ended with 3665225 records.\n",
      "  K-core filtering stabilized.\n",
      "K-core filtering complete. Records after filtering: 3665225\n",
      "\n",
      "Starting per-user splitting...\n",
      "  Train set interaction count (after per-user split): 2008955\n",
      "  Validation set interaction count (after per-user split): 785055\n",
      "  Test set interaction count (after per-user split): 871215\n",
      "\n",
      "Ensuring warm-start for validation and test sets (items must exist in train set)...\n",
      "  Number of unique items in the final training set: 63487\n",
      "  Validation interactions: Original = 785055, After ensuring items in train = 784397\n",
      "  INFO: 658 interactions removed from validation set because their item was not present in the training set.\n",
      "  Test interactions: Original = 871215, After ensuring items in train = 870421\n",
      "  INFO: 794 interactions removed from test set because their item was not present in the training set.\n",
      "\n",
      "Saving data sets to CSV (output will be directories)...\n",
      "Final counts for output: Train=2008955, Validation=784397, Test=870421\n",
      "  Training data saved to: ./als_split/train_data\n",
      "  Validation data saved to: ./als_split/validation_data\n",
      "  Test data saved to: ./als_split/test_data\n",
      "\n",
      "Data splitting and saving process complete.\n",
      "\n",
      "All cached DataFrames unpersisted.\n"
     ]
    }
   ],
   "source": [
    "# --- User Defined Parameters ---\n",
    "MIN_INTERACTIONS_PER_USER = 3\n",
    "MIN_INTERACTIONS_PER_ITEM = 3  # For businesses/restaurants\n",
    "TRAIN_RATIO = 0.6\n",
    "VALIDATION_RATIO = 0.2\n",
    "# TEST_RATIO is implicitly 1.0 - TRAIN_RATIO - VALIDATION_RATIO (i.e., 0.2)\n",
    "\n",
    "# Column names (ensure these match your DataFrame)\n",
    "USER_ID_COL = \"user_id\"\n",
    "ITEM_ID_COL = \"business_id\"\n",
    "# RATING_COL = \"stars\" # Assuming this is a column in your df, though not directly used in split logic here.\n",
    "\n",
    "TRAIN_CSV_PATH = \"./als_split/train_data\"\n",
    "VALIDATION_CSV_PATH = \"./als_split/validation_data\"\n",
    "TEST_CSV_PATH = \"./als_split/test_data\"\n",
    "\n",
    "# Assume 'als_input_df_loaded' is your PySpark DataFrame\n",
    "# For example:\n",
    "# als_input_df_loaded = spark.read.parquet(\"path/to/your/als_input_df_loaded.parquet\")\n",
    "\n",
    "print(f\"Original dataset record count: {als_input_df_loaded.count()}\")\n",
    "original_cols = distinct_combined_df.columns # Store original columns\n",
    "\n",
    "# --- Step 1: K-Core Filtering (Iterative) ---\n",
    "print(\"\\nStarting K-core filtering...\")\n",
    "current_df = als_input_df_loaded\n",
    "\n",
    "# Iteratively filter users and items until counts stabilize or max iterations are reached\n",
    "for i in range(10): # Max 10 iterations for k-core filtering\n",
    "    initial_count_iter = current_df.count()\n",
    "    print(f\"  K-core Iteration {i+1}: Starting with {initial_count_iter} records.\")\n",
    "\n",
    "    # Filter by min interactions per user\n",
    "    user_interaction_counts = current_df.groupBy(USER_ID_COL).agg(count(\"*\").alias(\"u_count\"))\n",
    "    df_after_user_filter = current_df.join(user_interaction_counts, USER_ID_COL, \"inner\") \\\n",
    "                                     .filter(col(\"u_count\") >= MIN_INTERACTIONS_PER_USER) \\\n",
    "                                     .select(*original_cols) # Keep only original columns to avoid duplicate columns\n",
    "\n",
    "    # Filter by min interactions per item (on the result of user filtering)\n",
    "    item_interaction_counts = df_after_user_filter.groupBy(ITEM_ID_COL).agg(count(\"*\").alias(\"i_count\"))\n",
    "    current_df = df_after_user_filter.join(item_interaction_counts, ITEM_ID_COL, \"inner\") \\\n",
    "                                       .filter(col(\"i_count\") >= MIN_INTERACTIONS_PER_ITEM) \\\n",
    "                                       .select(*original_cols) # Keep only original columns\n",
    "    \n",
    "    current_iter_count_after_filter = current_df.count()\n",
    "    print(f\"  K-core Iteration {i+1}: Ended with {current_iter_count_after_filter} records.\")\n",
    "    \n",
    "    if current_iter_count_after_filter == initial_count_iter:\n",
    "        print(\"  K-core filtering stabilized.\")\n",
    "        break\n",
    "    if i == 9: # Last iteration\n",
    "        print(\"  Warning: K-core filtering reached max iterations.\")\n",
    "\n",
    "filtered_df = current_df.cache() # Cache the result of k-core filtering\n",
    "final_filtered_count = filtered_df.count()\n",
    "print(f\"K-core filtering complete. Records after filtering: {final_filtered_count}\")\n",
    "\n",
    "if final_filtered_count == 0:\n",
    "    print(\"ERROR: K-core filtering resulted in an empty DataFrame. \")\n",
    "    print(\"Please check your MIN_INTERACTIONS thresholds or the density of your input data.\")\n",
    "    # Consider raising an error or exiting if this happens:\n",
    "    # raise ValueError(\"K-core filtering resulted in an empty DataFrame.\")\n",
    "    # For now, we'll print an error and let it continue, though subsequent steps will likely fail.\n",
    "\n",
    "# --- Step 2: Per-User Splitting ---\n",
    "# This step splits each user's interactions proportionally into train, validation, and test sets.\n",
    "print(\"\\nStarting per-user splitting...\")\n",
    "\n",
    "# Add a column with the total number of interactions for each user (post k-core filtering)\n",
    "df_with_user_total_interactions = filtered_df.withColumn(\n",
    "    \"total_interactions_for_user\", \n",
    "    count(\"*\").over(Window.partitionBy(USER_ID_COL))\n",
    ")\n",
    "\n",
    "# Define a window specification to order interactions randomly within each user group.\n",
    "# Using a fixed seed (e.g., 42) for rand() ensures reproducibility of the split.\n",
    "window_spec_user_split = Window.partitionBy(USER_ID_COL).orderBy(rand(seed=42))\n",
    "\n",
    "# Add a row number (rank) to each interaction within its user group\n",
    "df_with_rn = df_with_user_total_interactions.withColumn(\"rn_in_user\", row_number().over(window_spec_user_split))\n",
    "\n",
    "# Calculate the index boundaries for splitting based on defined ratios.\n",
    "# Using floor ensures that for the minimum interaction count (e.g., 3 for a user),\n",
    "# the items are distributed as 1 to train, 1 to validation, and 1 to test.\n",
    "df_with_boundaries = df_with_rn.withColumn(\n",
    "    \"train_boundary_idx\", \n",
    "    floor(TRAIN_RATIO * col(\"total_interactions_for_user\"))\n",
    ").withColumn(\n",
    "    \"validation_boundary_idx\",\n",
    "    floor((TRAIN_RATIO + VALIDATION_RATIO) * col(\"total_interactions_for_user\"))\n",
    ")\n",
    "\n",
    "# Assign records to train, validation, or test sets based on their row number and the calculated boundaries\n",
    "# For a user with MIN_INTERACTIONS_PER_USER (3):\n",
    "#   train_boundary_idx = floor(0.6 * 3) = 1\n",
    "#   validation_boundary_idx = floor(0.8 * 3) = 2\n",
    "#   Train: rn_in_user <= 1 (1 item)\n",
    "#   Validation: rn_in_user > 1 AND rn_in_user <= 2 (1 item)\n",
    "#   Test: rn_in_user > 2 (1 item)\n",
    "# This results in a 1, 1, 1 split for users with the minimum 3 interactions.\n",
    "\n",
    "train_df_intermediate = df_with_boundaries.filter(col(\"rn_in_user\") <= col(\"train_boundary_idx\"))\n",
    "validation_df_intermediate = df_with_boundaries.filter(\n",
    "    (col(\"rn_in_user\") > col(\"train_boundary_idx\")) & (col(\"rn_in_user\") <= col(\"validation_boundary_idx\"))\n",
    ")\n",
    "test_df_intermediate = df_with_boundaries.filter(col(\"rn_in_user\") > col(\"validation_boundary_idx\"))\n",
    "\n",
    "# Select only the original columns for the final DataFrames and cache them\n",
    "train_df = train_df_intermediate.select(*original_cols).cache()\n",
    "validation_df = validation_df_intermediate.select(*original_cols).cache()\n",
    "test_df = test_df_intermediate.select(*original_cols).cache()\n",
    "\n",
    "# Unpersist the DataFrame that results from k-core filtering as it's no longer directly needed\n",
    "filtered_df.unpersist()\n",
    "\n",
    "print(f\"  Train set interaction count (after per-user split): {train_df.count()}\")\n",
    "print(f\"  Validation set interaction count (after per-user split): {validation_df.count()}\")\n",
    "print(f\"  Test set interaction count (after per-user split): {test_df.count()}\")\n",
    "\n",
    "# --- Step 3: Ensure Items in Validation/Test are in Train (Warm-Start Evaluation) ---\n",
    "# This step ensures that items in the validation and test sets have also been seen in the training set.\n",
    "# This is a common practice for evaluating recommendation models fairly.\n",
    "print(\"\\nEnsuring warm-start for validation and test sets (items must exist in train set)...\")\n",
    "\n",
    "train_items = train_df.select(ITEM_ID_COL).distinct().cache()\n",
    "num_train_items = train_items.count()\n",
    "print(f\"  Number of unique items in the final training set: {num_train_items}\")\n",
    "\n",
    "# Filter validation set\n",
    "original_val_count = validation_df.count()\n",
    "# Perform an inner join with unique items from the training set\n",
    "validation_df_final = validation_df.join(train_items, ITEM_ID_COL, \"inner\").select(*original_cols).cache()\n",
    "new_val_count = validation_df_final.count()\n",
    "validation_df.unpersist() # Unpersist the version before this \"warm-start\" filtering\n",
    "print(f\"  Validation interactions: Original = {original_val_count}, After ensuring items in train = {new_val_count}\")\n",
    "if original_val_count > new_val_count:\n",
    "    print(f\"  INFO: {original_val_count - new_val_count} interactions removed from validation set because their item was not present in the training set.\")\n",
    "\n",
    "# Filter test set\n",
    "original_test_count = test_df.count()\n",
    "# Perform an inner join with unique items from the training set\n",
    "test_df_final = test_df.join(train_items, ITEM_ID_COL, \"inner\").select(*original_cols).cache()\n",
    "new_test_count = test_df_final.count()\n",
    "test_df.unpersist() # Unpersist the version before this \"warm-start\" filtering\n",
    "print(f\"  Test interactions: Original = {original_test_count}, After ensuring items in train = {new_test_count}\")\n",
    "if original_test_count > new_test_count:\n",
    "    print(f\"  INFO: {original_test_count - new_test_count} interactions removed from test set because their item was not present in the training set.\")\n",
    "\n",
    "train_items.unpersist() # Unpersist the distinct training items DataFrame\n",
    "\n",
    "# --- Step 4: Saving DataFrames to CSV ---\n",
    "print(\"\\nSaving data sets to CSV (output will be directories)...\")\n",
    "\n",
    "final_train_count = train_df.count() # train_df is already the final version for training\n",
    "final_validation_count = validation_df_final.count()\n",
    "final_test_count = test_df_final.count()\n",
    "\n",
    "print(f\"Final counts for output: Train={final_train_count}, Validation={final_validation_count}, Test={final_test_count}\")\n",
    "\n",
    "if final_train_count > 0:\n",
    "    train_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(TRAIN_CSV_PATH)\n",
    "    print(f\"  Training data saved to: {TRAIN_CSV_PATH}\")\n",
    "else:\n",
    "    print(f\"  Skipping saving training data as it is empty.\")\n",
    "\n",
    "if final_validation_count > 0:\n",
    "    validation_df_final.write.mode(\"overwrite\").option(\"header\", \"true\").csv(VALIDATION_CSV_PATH)\n",
    "    print(f\"  Validation data saved to: {VALIDATION_CSV_PATH}\")\n",
    "else:\n",
    "    print(f\"  Skipping saving validation data as it is empty.\")\n",
    "\n",
    "if final_test_count > 0:\n",
    "    test_df_final.write.mode(\"overwrite\").option(\"header\", \"true\").csv(TEST_CSV_PATH)\n",
    "    print(f\"  Test data saved to: {TEST_CSV_PATH}\")\n",
    "else:\n",
    "    print(f\"  Skipping saving test data as it is empty.\")\n",
    "\n",
    "print(\"\\nData splitting and saving process complete.\")\n",
    "\n",
    "# --- Clean up cached DataFrames ---\n",
    "train_df.unpersist()\n",
    "validation_df_final.unpersist()\n",
    "test_df_final.unpersist()\n",
    "\n",
    "print(\"\\nAll cached DataFrames unpersisted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
