{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f7eff0-3d8c-4070-8426-1ef170d09c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbb520f-1b70-46b9-aaa2-4467114fa335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created for training\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"ALSTraining\") \n",
    "    .master(\"local[*]\")  # Adjust as needed for your environment\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # Adjust as needed\n",
    "    .config(\"spark.executor.memory\", \"4g\") # Adjust as needed\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") # Adjust as needed\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session created for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfff682-1511-416b-976b-cb7e9c727ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|             user_id|     userCol|\n",
      "+--------------------+------------+\n",
      "|--2bpE5vyR-2hAP7s...|274877906944|\n",
      "|--T_QxqWcEu76n1da...|274877906945|\n",
      "|--q3Qv-yYG9jFqXlM...|274877906946|\n",
      "|-0BfVK9AA00ynhvW6...|274877906947|\n",
      "|-0bmx13qzWqXCafeA...|274877906948|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+------------+\n",
      "|         business_id|     itemCol|\n",
      "+--------------------+------------+\n",
      "|NAMen7YzwlYDs_5EC...|326417514496|\n",
      "|BjBDHqHhMXSxgyVip...|326417514497|\n",
      "|wm5mQ4cSpvko9WlCq...|326417514498|\n",
      "|KD9-X5AykKmiZszrM...|326417514499|\n",
      "|eLOqWp2OLfr5dfJHw...|326417514500|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "ID mappings loaded successfully\n"
     ]
    }
   ],
   "source": [
    "user_id_mapping_path = \"/home/jovyan/work/id_mappings/user_id_mapping\"\n",
    "user_id_mapping_loaded = spark.read.parquet(user_id_mapping_path)\n",
    "user_id_mapping_loaded.show(5)\n",
    "\n",
    "business_id_mapping_path = \"/home/jovyan/work/id_mappings/business_id_mapping\"\n",
    "business_id_mapping_loaded = spark.read.parquet(business_id_mapping_path)\n",
    "business_id_mapping_loaded.show(5)\n",
    "\n",
    "print(\"ID mappings loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e9f09a-d20b-44d5-90e5-2b04381ba60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userCol: integer (nullable = true)\n",
      " |-- itemCol: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      "\n",
      "+-------+-------+------+\n",
      "|userCol|itemCol|rating|\n",
      "+-------+-------+------+\n",
      "|      0|    203|   1.0|\n",
      "|      1|    215|   5.0|\n",
      "|     89|    398|   4.0|\n",
      "|      2|      6|   5.0|\n",
      "|      3|    120|   1.0|\n",
      "+-------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "ALS input data loaded and prepared\n"
     ]
    }
   ],
   "source": [
    "file_path = \"training_data.csv\"  # Replace with the actual path to your data\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "als_input_df_loaded = df.join(user_id_mapping_loaded, \"user_id\", \"inner\").drop(\"user_id\")\n",
    "als_input_df_loaded = als_input_df_loaded.join(business_id_mapping_loaded, \"business_id\", \"inner\").drop(\"business_id\")\n",
    "\n",
    "als_input_df_loaded = als_input_df_loaded.select(\n",
    "    col(\"userCol\").cast(\"int\"),\n",
    "    col(\"itemCol\").cast(\"int\"),\n",
    "    col(\"stars\").cast(\"float\").alias(\"rating\") # ALS expects 'rating' column\n",
    ")\n",
    "\n",
    "als_input_df_loaded.printSchema()\n",
    "als_input_df_loaded.show(5)\n",
    "\n",
    "print(\"ALS input data loaded and prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e698462c-1d92-4da9-85cb-b9471f7069a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model trained successfully\n"
     ]
    }
   ],
   "source": [
    "als = ALS(userCol=\"userCol\",\n",
    "          itemCol=\"itemCol\",\n",
    "          ratingCol=\"rating\",\n",
    "          rank=10,          # Number of latent factors\n",
    "          maxIter=10,       # Maximum number of iterations\n",
    "          regParam=0.01,    # Regularization parameter\n",
    "          coldStartStrategy=\"drop\") # Handle new users/items\n",
    "\n",
    "# Train the model\n",
    "model = als.fit(als_input_df_loaded)\n",
    "\n",
    "print(\"ALS model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433168a9-dcf3-4291-ac6d-3985f5a27343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|userCol|            features|             user_id|\n",
      "+-------+--------------------+--------------------+\n",
      "|     26|[0.44952798, -3.1...|-EmuvqfmhKylSVG1H...|\n",
      "|     29|[0.70582867, -2.7...|-FybaJ4pQZsfoRfW5...|\n",
      "|    474|[0.68184626, -3.1...|-GsRfCDYv0myI_YCv...|\n",
      "|    964|[-0.12040462, -3....|0LhRYfa7YErm-Y0Xh...|\n",
      "|   1677|[1.041442, -2.852...|22ml-CTcoabnc-uu4...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|itemCol|            features|         business_id|\n",
      "+-------+--------------------+--------------------+\n",
      "|      0|[0.32506737, -0.3...|vXb4OWsjPoiBtmarf...|\n",
      "|     10|[0.009884086, -0....|Q9G2gtnVDZgsNerHD...|\n",
      "|     20|[0.0032310067, -0...|LnJSsNVZkStgtj86f...|\n",
      "|     30|[0.5399539, -0.17...|bQ_R0bLvTWS4jHsF6...|\n",
      "|     40|[-0.21802746, -0....|jzsspHqP9kATWji8x...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get user latent factors with original user IDs\n",
    "user_latent_vectors = model.userFactors.withColumnRenamed(\"id\", \"userCol\").join(user_id_mapping_loaded, \"userCol\", \"inner\")\n",
    "user_latent_vectors.show(5)\n",
    "\n",
    "# Get item latent factors with original business IDs\n",
    "item_latent_vectors = model.itemFactors.withColumnRenamed(\"id\", \"itemCol\").join(business_id_mapping_loaded, \"itemCol\", \"inner\")\n",
    "item_latent_vectors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c147233-3c2a-4227-86b1-1ad99e3dc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "# Convert the 'features' column to a string before writing to CSV for users\n",
    "user_latent_vectors_string = user_latent_vectors.withColumn(\"features\", concat_ws(\",\", col(\"features\")))\n",
    "\n",
    "# Export user_latent_vectors to a CSV file\n",
    "user_latent_vectors_string.coalesce(1).write.csv(\"user_latent_vectors.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Convert the 'features' column to a string before writing to CSV for businesses\n",
    "item_latent_vectors_string = item_latent_vectors.withColumn(\"features\", concat_ws(\",\", col(\"features\")))\n",
    "\n",
    "# Export item_latent_vectors to a CSV file\n",
    "item_latent_vectors_string.coalesce(1).write.csv(\"item_latent_vectors.csv\", header=True, mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
