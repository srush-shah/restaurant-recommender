{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a0f9630-4524-4751-925e-68e180dc749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dadf74e6-7041-4040-8735-3443e0a8d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_VECTOR_CSV_STRING_ID_COL = 'user_id'\n",
    "USER_VECTOR_CSV_VECTOR_STRING_COL = 'features'\n",
    "\n",
    "ITEM_VECTOR_CSV_STRING_ID_COL = 'business_id'\n",
    "ITEM_VECTOR_CSV_VECTOR_STRING_COL = 'features'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d553bc-5af8-48b9-9028-c6ec2c1a02fd",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045b80c9-fdd5-4432-9fb7-b5fcfb752e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df_main columns: ['user_id', 'business_id', 'stars', 'date', 'city', 'category_0', 'category_1', 'category_2', 'category_3', 'category_4', 'category_5', 'category_6', 'category_7', 'category_8', 'category_9', 'category_10', 'category_11', 'category_12', 'category_13', 'category_14', 'category_15', 'category_16', 'category_17', 'category_18', 'category_19', 'category_20', 'category_21', 'category_22', 'category_23', 'category_24', 'category_25', 'category_26', 'category_27', 'category_28', 'category_29', 'category_30', 'category_31', 'category_32', 'category_33', 'category_34', 'category_35', 'category_36', 'category_37', 'category_38', 'category_39', 'category_40', 'category_41', 'category_42', 'category_43', 'category_44', 'category_45', 'category_46', 'category_47', 'category_48', 'category_49', 'category_50', 'category_51', 'category_52', 'category_53', 'category_54', 'category_55', 'category_56', 'category_57', 'category_58', 'category_59', 'category_60', 'category_61', 'category_62', 'category_63', 'category_64', 'category_65', 'category_66', 'category_67', 'category_68', 'category_69', 'category_70', 'category_71', 'category_72', 'category_73', 'category_74', 'category_75', 'category_76', 'category_77', 'category_78', 'category_79', 'category_80', 'category_81', 'category_82', 'category_83', 'category_84', 'category_85', 'category_86', 'category_87', 'category_88', 'category_89', 'category_90', 'category_91', 'category_92', 'category_93', 'category_94', 'category_95', 'category_96', 'category_97', 'category_98', 'category_99', 'category_100', 'category_101', 'category_102', 'category_103', 'category_104', 'category_105', 'category_106', 'category_107', 'category_108', 'category_109', 'category_110', 'category_111', 'category_112', 'category_113', 'category_114', 'category_115', 'category_116', 'category_117', 'category_118', 'category_119', 'category_120', 'category_121', 'category_122', 'category_123', 'category_124', 'category_125', 'category_126', 'category_127', 'category_128', 'category_129', 'category_130', 'category_131', 'category_132', 'category_133', 'category_134', 'category_135', 'category_136', 'category_137', 'category_138', 'category_139', 'category_140', 'category_141', 'category_142', 'category_143', 'category_144', 'category_145', 'category_146', 'category_147', 'category_148', 'category_149', 'category_150', 'category_151', 'category_152', 'category_153', 'category_154', 'category_155', 'category_156', 'category_157', 'category_158', 'category_159', 'category_160', 'category_161', 'category_162', 'category_163', 'category_164', 'category_165', 'category_166', 'category_167', 'category_168', 'category_169', 'category_170', 'category_171', 'category_172', 'category_173', 'category_174', 'category_175', 'category_176', 'category_177', 'category_178', 'category_179', 'category_180', 'category_181', 'category_182', 'category_183', 'category_184', 'category_185', 'category_186', 'category_187', 'category_188', 'category_189', 'category_190', 'category_191', 'category_192', 'category_193', 'category_194', 'category_195', 'category_196', 'category_197', 'category_198', 'category_199', 'category_200', 'category_201', 'category_202', 'category_203', 'category_204', 'category_205', 'category_206', 'category_207', 'category_208', 'category_209', 'category_210', 'category_211', 'category_212', 'category_213', 'category_214', 'category_215', 'category_216', 'category_217', 'category_218', 'category_219', 'category_220', 'category_221', 'category_222', 'category_223', 'category_224', 'category_225', 'category_226', 'category_227', 'category_228', 'category_229', 'category_230', 'category_231', 'category_232', 'category_233', 'category_234', 'category_235', 'category_236', 'category_237', 'category_238', 'category_239', 'category_240', 'category_241', 'category_242', 'category_243', 'category_244', 'category_245', 'category_246', 'category_247', 'category_248', 'category_249', 'category_250', 'category_251', 'category_252', 'category_253', 'category_254', 'category_255', 'category_256', 'category_257', 'category_258', 'category_259', 'category_260', 'category_261', 'category_262', 'category_263', 'category_264', 'category_265', 'category_266', 'category_267', 'category_268', 'category_269', 'category_270', 'category_271', 'category_272', 'category_273', 'category_274', 'category_275', 'category_276', 'category_277', 'category_278', 'category_279', 'category_280', 'category_281', 'category_282', 'category_283', 'category_284', 'category_285', 'category_286', 'category_287', 'category_288', 'category_289', 'category_290', 'category_291', 'category_292', 'category_293', 'category_294', 'category_295', 'category_296', 'category_297', 'category_298', 'category_299', 'category_300', 'category_301', 'category_302', 'category_303', 'category_304', 'category_305', 'category_306', 'category_307', 'category_308', 'category_309', 'category_310', 'category_311', 'category_312', 'category_313', 'category_314', 'category_315', 'category_316', 'category_317', 'category_318', 'category_319', 'category_320', 'category_321', 'category_322', 'category_323', 'category_324', 'category_325', 'category_326', 'category_327', 'category_328', 'category_329', 'category_330', 'category_331', 'category_332', 'category_333', 'category_334', 'category_335', 'category_336', 'category_337', 'category_338', 'category_339', 'category_340', 'category_341', 'category_342', 'category_343', 'category_344', 'category_345', 'category_346', 'category_347', 'category_348', 'category_349', 'category_350', 'category_351', 'category_352', 'category_353', 'category_354', 'category_355', 'category_356', 'category_357', 'category_358', 'category_359', 'category_360', 'category_361', 'category_362', 'category_363', 'category_364', 'category_365', 'category_366', 'category_367', 'category_368', 'category_369', 'category_370', 'category_371', 'category_372', 'category_373', 'category_374', 'category_375', 'category_376', 'category_377', 'category_378', 'category_379', 'category_380', 'category_381', 'category_382', 'category_383', 'category_384', 'category_385', 'category_386', 'category_387', 'category_388', 'category_389', 'category_390', 'category_391', 'category_392', 'category_393', 'category_394', 'category_395', 'category_396', 'category_397', 'category_398', 'category_399', 'category_400', 'category_401']\n",
      "Raw user_vectors_df_raw (loaded specific cols) columns: ['features', 'user_id']\n",
      "Sample user vector string: -0.3945538,0.002257109,0.39395103,-0.5069559,0.0072146826,-0.5091034,1.1947032,0.05169414,-0.015809959,0.4989379\n",
      "Raw item_vectors_df_raw (loaded specific cols) columns: ['features', 'business_id']\n",
      "Sample item vector string: -1.2053361,0.21339719,0.1252676,-0.24372746,-0.21597369,-0.7387609,2.3786557,-0.23264037,1.0191052,0.8935229\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_main = pd.read_csv('./data/als/training_data.csv')\n",
    "\n",
    "    # Load only the necessary columns from the vector CSVs\n",
    "    user_vectors_df_raw = pd.read_csv(\n",
    "        'user-latent-vectors.csv',\n",
    "        usecols=[USER_VECTOR_CSV_STRING_ID_COL, USER_VECTOR_CSV_VECTOR_STRING_COL],\n",
    "        dtype={USER_VECTOR_CSV_VECTOR_STRING_COL: str}\n",
    "    )\n",
    "    item_vectors_df_raw = pd.read_csv(\n",
    "        'item-latent-vectors.csv',\n",
    "        usecols=[ITEM_VECTOR_CSV_STRING_ID_COL, ITEM_VECTOR_CSV_VECTOR_STRING_COL],\n",
    "        dtype={ITEM_VECTOR_CSV_VECTOR_STRING_COL: str}\n",
    "    )\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "    raise\n",
    "except ValueError as e: # Handles errors from usecols if a specified column doesn't exist\n",
    "    print(f\"ValueError during CSV loading. Check if your specified ID and vector string columns exist in the CSVs: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Original df_main columns:\", df_main.columns.tolist())\n",
    "print(f\"Raw user_vectors_df_raw (loaded specific cols) columns: {user_vectors_df_raw.columns.tolist()[:5]}\")\n",
    "if not user_vectors_df_raw.empty:\n",
    "    print(f\"Sample user vector string: {user_vectors_df_raw[USER_VECTOR_CSV_VECTOR_STRING_COL].iloc[0]}\")\n",
    "print(f\"Raw item_vectors_df_raw (loaded specific cols) columns: {item_vectors_df_raw.columns.tolist()}\")\n",
    "if not item_vectors_df_raw.empty:\n",
    "    print(f\"Sample item vector string: {item_vectors_df_raw[ITEM_VECTOR_CSV_VECTOR_STRING_COL].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab94de6-6be7-41a5-82bb-04ae2e35bf30",
   "metadata": {},
   "source": [
    "### Parse and Expand Single Vector Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0c5788-3097-46db-85a3-a50b0d05e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing user vectors. ID col for merge: 'user_id', Vector string col: 'features'\n",
      "\n",
      "Processing item vectors. ID col for merge: 'business_id', Vector string col: 'features'\n"
     ]
    }
   ],
   "source": [
    "def parse_comma_separated_float_string(vector_string):\n",
    "    \"\"\"\n",
    "    Parses a string of comma-separated float values.\n",
    "    Example input: \"-0.375,0.948,...\"\n",
    "    Returns a list of floats, or an empty list if parsing fails or input is invalid.\n",
    "    \"\"\"\n",
    "    if pd.isna(vector_string) or not isinstance(vector_string, str) or not vector_string.strip():\n",
    "        return []  # Handle NaN, non-string, or empty/whitespace-only strings\n",
    "    try:\n",
    "        # Split by comma, strip whitespace from each part, convert to float\n",
    "        return [float(x.strip()) for x in vector_string.split(',')]\n",
    "    except ValueError:\n",
    "        # This will catch cases where a part is not a valid float\n",
    "        # You could log the problematic string here if needed: print(f\"Could not parse: {vector_string}\")\n",
    "        return []\n",
    "\n",
    "def parse_and_expand_vector_column(df_raw, id_col_name_for_merge, single_vec_str_col_name, vec_prefix):\n",
    "    if df_raw.empty or single_vec_str_col_name not in df_raw.columns:\n",
    "        print(f\"Warning: DataFrame for '{vec_prefix}' is empty or vector string column '{single_vec_str_col_name}' not found.\")\n",
    "        return pd.DataFrame(columns=[id_col_name_for_merge])\n",
    "\n",
    "    # Apply the new parsing function\n",
    "    parsed_vectors = df_raw[single_vec_str_col_name].apply(parse_comma_separated_float_string)\n",
    "\n",
    "    # Create a DataFrame from the list of lists (each sublist is a vector)\n",
    "    vector_components_df = pd.DataFrame(parsed_vectors.tolist(), index=df_raw.index)\n",
    "\n",
    "    if vector_components_df.empty and not parsed_vectors.empty:\n",
    "        # This case might happen if all parsed_vectors resulted in empty lists (e.g. all input strings were invalid)\n",
    "        # Try to infer dimension if possible, or create an empty df with specific columns if dim is known\n",
    "        print(f\"Warning: All vectors for '{vec_prefix}' were empty or unparseable. No component columns created from data.\")\n",
    "        # If you know the expected dimension, you could create zero columns here:\n",
    "        # EXPECTED_DIM = 10 # For example\n",
    "        # vector_components_df = pd.DataFrame(np.zeros((len(df_raw), EXPECTED_DIM)), index=df_raw.index)\n",
    "        # For now, let it proceed; if it has no columns, subsequent steps will show it.\n",
    "\n",
    "    if not vector_components_df.empty:\n",
    "        # Name new columns if components were successfully created\n",
    "        vector_components_df.columns = [f\"{vec_prefix}{i}\" for i in range(vector_components_df.shape[1])]\n",
    "        # Ensure all components are numeric (should be, due to float conversion in parsing)\n",
    "        # and fill any NaNs (though parsing should return empty list on error, not NaNs in the list)\n",
    "        vector_components_df = vector_components_df.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "    else:\n",
    "        print(f\"Warning: No vector component columns created for '{vec_prefix}' because parsed data led to empty DataFrame.\")\n",
    "\n",
    "    expanded_df = pd.concat([df_raw[[id_col_name_for_merge]], vector_components_df], axis=1)\n",
    "    return expanded_df\n",
    "\n",
    "# Apply the parsing and expansion\n",
    "print(f\"\\nProcessing user vectors. ID col for merge: '{USER_VECTOR_CSV_STRING_ID_COL}', Vector string col: '{USER_VECTOR_CSV_VECTOR_STRING_COL}'\")\n",
    "user_vectors_expanded_df = parse_and_expand_vector_column(\n",
    "    user_vectors_df_raw,\n",
    "    USER_VECTOR_CSV_STRING_ID_COL,\n",
    "    USER_VECTOR_CSV_VECTOR_STRING_COL,\n",
    "    \"user_vec_\"\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessing item vectors. ID col for merge: '{ITEM_VECTOR_CSV_STRING_ID_COL}', Vector string col: '{ITEM_VECTOR_CSV_VECTOR_STRING_COL}'\")\n",
    "item_vectors_expanded_df = parse_and_expand_vector_column(\n",
    "    item_vectors_df_raw,\n",
    "    ITEM_VECTOR_CSV_STRING_ID_COL,\n",
    "    ITEM_VECTOR_CSV_VECTOR_STRING_COL,\n",
    "    \"item_vec_\"\n",
    ")\n",
    "\n",
    "# print(\"\\nExpanded user_vectors_df head:\")\n",
    "# print(user_vectors_expanded_df.head().iloc[:, :min(5, user_vectors_expanded_df.shape[1])])\n",
    "# print(\"\\nExpanded item_vectors_df head:\")\n",
    "# print(item_vectors_expanded_df.head().iloc[:, :min(5, item_vectors_expanded_df.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a4697-c091-49a2-8742-60a57a27a287",
   "metadata": {},
   "source": [
    "### Merge expanded latent vectors with the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5c467d-c689-4b3c-b6e8-18db502fee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DF_USER_ID_COL = 'user_id'\n",
    "MAIN_DF_ITEM_ID_COL = 'business_id'\n",
    "\n",
    "df_merged = pd.merge(df_main, user_vectors_expanded_df,\n",
    "                     left_on=MAIN_DF_USER_ID_COL, right_on=USER_VECTOR_CSV_STRING_ID_COL,\n",
    "                     how='left')\n",
    "if MAIN_DF_USER_ID_COL != USER_VECTOR_CSV_STRING_ID_COL and USER_VECTOR_CSV_STRING_ID_COL in df_merged.columns:\n",
    "    df_merged.drop(columns=[USER_VECTOR_CSV_STRING_ID_COL], inplace=True)\n",
    "\n",
    "df_merged = pd.merge(df_merged, item_vectors_expanded_df,\n",
    "                     left_on=MAIN_DF_ITEM_ID_COL, right_on=ITEM_VECTOR_CSV_STRING_ID_COL,\n",
    "                     how='left')\n",
    "if MAIN_DF_ITEM_ID_COL != ITEM_VECTOR_CSV_STRING_ID_COL and ITEM_VECTOR_CSV_STRING_ID_COL in df_merged.columns:\n",
    "    df_merged.drop(columns=[ITEM_VECTOR_CSV_STRING_ID_COL], inplace=True)\n",
    "df_merged = df_merged.fillna(0.0)\n",
    "# print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c1cff62-f823-4d21-9685-2390e71f17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model_input = df_merged.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03260e6d-19dc-4497-916f-d5e30c3a7fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>category_6</th>\n",
       "      <th>category_7</th>\n",
       "      <th>category_8</th>\n",
       "      <th>...</th>\n",
       "      <th>item_vec_0</th>\n",
       "      <th>item_vec_1</th>\n",
       "      <th>item_vec_2</th>\n",
       "      <th>item_vec_3</th>\n",
       "      <th>item_vec_4</th>\n",
       "      <th>item_vec_5</th>\n",
       "      <th>item_vec_6</th>\n",
       "      <th>item_vec_7</th>\n",
       "      <th>item_vec_8</th>\n",
       "      <th>item_vec_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071201</td>\n",
       "      <td>0.189074</td>\n",
       "      <td>0.295039</td>\n",
       "      <td>-1.02415</td>\n",
       "      <td>-0.203889</td>\n",
       "      <td>-0.037743</td>\n",
       "      <td>2.024522</td>\n",
       "      <td>-0.277157</td>\n",
       "      <td>0.153012</td>\n",
       "      <td>1.786736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 423 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars  category_0  category_1  category_2  category_3  category_4  \\\n",
       "0      4         0.0         0.0         0.0         0.0         0.0   \n",
       "1      3         0.0         0.0         0.0         0.0         0.0   \n",
       "2      5         0.0         0.0         1.0         0.0         0.0   \n",
       "3      5         0.0         0.0         0.0         0.0         0.0   \n",
       "4      4         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   category_5  category_6  category_7  category_8  ...  item_vec_0  \\\n",
       "0         0.0         0.0         0.0         1.0  ...    0.000000   \n",
       "1         0.0         0.0         0.0         0.0  ...    0.000000   \n",
       "2         0.0         0.0         0.0         0.0  ...    0.000000   \n",
       "3         0.0         0.0         0.0         0.0  ...    0.000000   \n",
       "4         0.0         0.0         0.0         0.0  ...   -1.071201   \n",
       "\n",
       "   item_vec_1  item_vec_2  item_vec_3  item_vec_4  item_vec_5  item_vec_6  \\\n",
       "0    0.000000    0.000000     0.00000    0.000000    0.000000    0.000000   \n",
       "1    0.000000    0.000000     0.00000    0.000000    0.000000    0.000000   \n",
       "2    0.000000    0.000000     0.00000    0.000000    0.000000    0.000000   \n",
       "3    0.000000    0.000000     0.00000    0.000000    0.000000    0.000000   \n",
       "4    0.189074    0.295039    -1.02415   -0.203889   -0.037743    2.024522   \n",
       "\n",
       "   item_vec_7  item_vec_8  item_vec_9  \n",
       "0    0.000000    0.000000    0.000000  \n",
       "1    0.000000    0.000000    0.000000  \n",
       "2    0.000000    0.000000    0.000000  \n",
       "3    0.000000    0.000000    0.000000  \n",
       "4   -0.277157    0.153012    1.786736  \n",
       "\n",
       "[5 rows x 423 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_model_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1360a5-f27e-4326-9a9a-c4de81e371ce",
   "metadata": {},
   "source": [
    "### Perform train test split on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9044950e-995e-477b-8a10-ec8e53c5ca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (650214, 423)\n",
      "Validation set shape: (139332, 423)\n",
      "Test set shape: (139332, 423)\n"
     ]
    }
   ],
   "source": [
    "# First, split into training and a temporary set (validation + test)\n",
    "df_train, df_temp = train_test_split(X_model_input, test_size=0.3, random_state=42)\n",
    "\n",
    "# Then, split the temporary set into validation and test\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {df_train.shape}\")\n",
    "print(f\"Validation set shape: {df_val.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "911834e3-b1d8-44fe-9efd-c5d4090f9830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>category_6</th>\n",
       "      <th>category_7</th>\n",
       "      <th>category_8</th>\n",
       "      <th>...</th>\n",
       "      <th>item_vec_0</th>\n",
       "      <th>item_vec_1</th>\n",
       "      <th>item_vec_2</th>\n",
       "      <th>item_vec_3</th>\n",
       "      <th>item_vec_4</th>\n",
       "      <th>item_vec_5</th>\n",
       "      <th>item_vec_6</th>\n",
       "      <th>item_vec_7</th>\n",
       "      <th>item_vec_8</th>\n",
       "      <th>item_vec_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626012</td>\n",
       "      <td>0.196008</td>\n",
       "      <td>-0.098658</td>\n",
       "      <td>-1.04249</td>\n",
       "      <td>-0.205068</td>\n",
       "      <td>-1.493916</td>\n",
       "      <td>2.220613</td>\n",
       "      <td>0.257302</td>\n",
       "      <td>1.132956</td>\n",
       "      <td>0.715663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759183</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772822</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835456</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476990</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 423 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stars  category_0  category_1  category_2  category_3  category_4  \\\n",
       "65321       3         0.0         0.0         0.0         0.0         0.0   \n",
       "759183      5         0.0         0.0         0.0         0.0         0.0   \n",
       "772822      3         0.0         0.0         0.0         0.0         0.0   \n",
       "835456      5         0.0         0.0         0.0         0.0         0.0   \n",
       "476990      3         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "        category_5  category_6  category_7  category_8  ...  item_vec_0  \\\n",
       "65321          0.0         0.0         0.0         0.0  ...   -0.626012   \n",
       "759183         0.0         0.0         0.0         0.0  ...    0.000000   \n",
       "772822         0.0         0.0         1.0         0.0  ...    0.000000   \n",
       "835456         0.0         0.0         0.0         0.0  ...    0.000000   \n",
       "476990         0.0         0.0         0.0         0.0  ...    0.000000   \n",
       "\n",
       "        item_vec_1  item_vec_2  item_vec_3  item_vec_4  item_vec_5  \\\n",
       "65321     0.196008   -0.098658    -1.04249   -0.205068   -1.493916   \n",
       "759183    0.000000    0.000000     0.00000    0.000000    0.000000   \n",
       "772822    0.000000    0.000000     0.00000    0.000000    0.000000   \n",
       "835456    0.000000    0.000000     0.00000    0.000000    0.000000   \n",
       "476990    0.000000    0.000000     0.00000    0.000000    0.000000   \n",
       "\n",
       "        item_vec_6  item_vec_7  item_vec_8  item_vec_9  \n",
       "65321     2.220613    0.257302    1.132956    0.715663  \n",
       "759183    0.000000    0.000000    0.000000    0.000000  \n",
       "772822    0.000000    0.000000    0.000000    0.000000  \n",
       "835456    0.000000    0.000000    0.000000    0.000000  \n",
       "476990    0.000000    0.000000    0.000000    0.000000  \n",
       "\n",
       "[5 rows x 423 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d78452-7199-4567-9e05-002cbd91504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes after separating X and y:\n",
      "X_train shape: (650214, 422), y_train shape: (650214,)\n",
      "X_val shape: (139332, 422), y_val shape: (139332,)\n",
      "X_test shape: (139332, 422), y_test shape: (139332,)\n",
      "\n",
      "NaNs in X_train: 0\n",
      "NaNs in X_val: 0\n",
      "NaNs in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "all_cols = df_train.columns.tolist()\n",
    "\n",
    "col_exclude = 'stars'\n",
    "\n",
    "model_feature_columns = [col for col in all_cols if col not in col_exclude]\n",
    "\n",
    "# Training set\n",
    "X_train = df_train[model_feature_columns]\n",
    "y_train = df_train['stars'] # Use the scaled target\n",
    "\n",
    "# Validation set\n",
    "X_val = df_val[model_feature_columns]\n",
    "y_val = df_val['stars']   # Use the scaled target\n",
    "\n",
    "# Test set\n",
    "X_test = df_test[model_feature_columns]\n",
    "y_test = df_test['stars']   # Use the scaled target\n",
    "\n",
    "print(\"\\nShapes after separating X and y:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Verify no NaNs in the final X sets (especially important after scaling/transforms)\n",
    "print(f\"\\nNaNs in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"NaNs in X_val: {X_val.isnull().sum().sum()}\")\n",
    "print(f\"NaNs in X_test: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "268348fc-d9cf-4b28-8e54-d0cf86286c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data to tensors...\n",
      "Data tensor conversion complete.\n",
      "Creating DataLoaders with batch_size=2048 and num_workers=4...\n",
      "DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch Tensors\n",
    "print(\"Converting data to tensors...\")\n",
    "# Training set\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Validation set\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Test set\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "print(\"Data tensor conversion complete.\")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 2048 # Adjust based on V100 VRAM and dataset size\n",
    "num_data_workers = 4 # Good starting point for V100\n",
    "\n",
    "print(f\"Creating DataLoaders with batch_size={batch_size} and num_workers={num_data_workers}...\")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # Shuffle training data\n",
    "    num_workers=num_data_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader( # Validation loader\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, # No need to shuffle validation or test data\n",
    "    num_workers=num_data_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader( # Test loader\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_data_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3217a4-cef5-4ec7-81af-d06e6f0481bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_feature_dim, hidden_dim_1, hidden_dim_2, output_size=1): \n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_feature_dim, hidden_dim_1)\n",
    "        self.relu1 = nn.ReLU()                        \n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_logits = nn.Linear(hidden_dim_2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        logits = self.output_logits(x)\n",
    "        \n",
    "        # Scale sigmoid output to be between 0 and 5\n",
    "        rating = 5.0 * torch.sigmoid(logits)\n",
    "        return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5bae21b-bd7b-4ca0-a912-96556d11bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating model with two hidden layers...\n",
      "CUDA device selected: Tesla V100-PCIE-16GB\n",
      "Model moved to device: cuda\n",
      "Input feature dimension: 422\n",
      "Model architecture: SimpleMLP(\n",
      "  (fc1): Linear(in_features=422, out_features=256, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (output_logits): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_1_size = 256\n",
    "hidden_layer_2_size = 128\n",
    "output_dim = 1\n",
    "\n",
    "print(\"Instantiating model with two hidden layers...\")\n",
    "model = SimpleMLP(input_feature_dim=input_dim,\n",
    "                  hidden_dim_1=hidden_layer_1_size,  # Matches __init__\n",
    "                  hidden_dim_2=hidden_layer_2_size,  # Matches __init__\n",
    "                  output_size=output_dim)\n",
    "\n",
    "# --- The rest of your Part 4 code remains the same ---\n",
    "criterion = nn.MSELoss() # Mean Squared Error for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not torch.cuda.is_available() and device.type == \"cuda\":\n",
    "    print(\"WARNING: CUDA not available, falling back to CPU!\")\n",
    "elif device.type == \"cuda\":\n",
    "    print(f\"CUDA device selected: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model moved to device: {device}\")\n",
    "print(f\"Input feature dimension: {input_dim}\")\n",
    "print(f\"Model architecture: {model}\") # This will now show both fc1, fc2, and output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbedd820-45d2-46f7-8251-ab7e6ac536ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20 epochs...\n",
      "Epoch [1/20], Training Loss (MSE): 1.2101, Time: 5.29s\n",
      "Epoch [1/20], Validation Loss (MSE): 1.2671, Validation RMSE: 1.1256, Validation Accuracy (Rounded): 32.71%\n",
      "--------------------------------------------------\n",
      "Epoch [2/20], Training Loss (MSE): 1.2080, Time: 5.37s\n",
      "Epoch [2/20], Validation Loss (MSE): 1.2637, Validation RMSE: 1.1242, Validation Accuracy (Rounded): 33.43%\n",
      "--------------------------------------------------\n",
      "Epoch [3/20], Training Loss (MSE): 1.2061, Time: 5.26s\n",
      "Epoch [3/20], Validation Loss (MSE): 1.2627, Validation RMSE: 1.1237, Validation Accuracy (Rounded): 33.49%\n",
      "--------------------------------------------------\n",
      "Epoch [4/20], Training Loss (MSE): 1.2038, Time: 5.32s\n",
      "Epoch [4/20], Validation Loss (MSE): 1.2644, Validation RMSE: 1.1245, Validation Accuracy (Rounded): 33.42%\n",
      "--------------------------------------------------\n",
      "Epoch [5/20], Training Loss (MSE): 1.2023, Time: 5.15s\n",
      "Epoch [5/20], Validation Loss (MSE): 1.2631, Validation RMSE: 1.1239, Validation Accuracy (Rounded): 33.46%\n",
      "--------------------------------------------------\n",
      "Epoch [6/20], Training Loss (MSE): 1.2002, Time: 5.26s\n",
      "Epoch [6/20], Validation Loss (MSE): 1.2656, Validation RMSE: 1.1250, Validation Accuracy (Rounded): 33.38%\n",
      "--------------------------------------------------\n",
      "Epoch [7/20], Training Loss (MSE): 1.1984, Time: 5.28s\n",
      "Epoch [7/20], Validation Loss (MSE): 1.2688, Validation RMSE: 1.1264, Validation Accuracy (Rounded): 32.97%\n",
      "--------------------------------------------------\n",
      "Epoch [8/20], Training Loss (MSE): 1.1966, Time: 5.21s\n",
      "Epoch [8/20], Validation Loss (MSE): 1.2637, Validation RMSE: 1.1241, Validation Accuracy (Rounded): 33.37%\n",
      "--------------------------------------------------\n",
      "Epoch [9/20], Training Loss (MSE): 1.1950, Time: 5.26s\n",
      "Epoch [9/20], Validation Loss (MSE): 1.2654, Validation RMSE: 1.1249, Validation Accuracy (Rounded): 33.63%\n",
      "--------------------------------------------------\n",
      "Epoch [10/20], Training Loss (MSE): 1.1936, Time: 5.17s\n",
      "Epoch [10/20], Validation Loss (MSE): 1.2651, Validation RMSE: 1.1247, Validation Accuracy (Rounded): 33.62%\n",
      "--------------------------------------------------\n",
      "Epoch [11/20], Training Loss (MSE): 1.1927, Time: 5.31s\n",
      "Epoch [11/20], Validation Loss (MSE): 1.2679, Validation RMSE: 1.1260, Validation Accuracy (Rounded): 33.29%\n",
      "--------------------------------------------------\n",
      "Epoch [12/20], Training Loss (MSE): 1.1914, Time: 5.15s\n",
      "Epoch [12/20], Validation Loss (MSE): 1.2677, Validation RMSE: 1.1259, Validation Accuracy (Rounded): 33.36%\n",
      "--------------------------------------------------\n",
      "Epoch [13/20], Training Loss (MSE): 1.1896, Time: 5.10s\n",
      "Epoch [13/20], Validation Loss (MSE): 1.2697, Validation RMSE: 1.1268, Validation Accuracy (Rounded): 33.79%\n",
      "--------------------------------------------------\n",
      "Epoch [14/20], Training Loss (MSE): 1.1889, Time: 5.23s\n",
      "Epoch [14/20], Validation Loss (MSE): 1.2722, Validation RMSE: 1.1279, Validation Accuracy (Rounded): 33.44%\n",
      "--------------------------------------------------\n",
      "Epoch [15/20], Training Loss (MSE): 1.1873, Time: 5.30s\n",
      "Epoch [15/20], Validation Loss (MSE): 1.2695, Validation RMSE: 1.1267, Validation Accuracy (Rounded): 33.86%\n",
      "--------------------------------------------------\n",
      "Epoch [16/20], Training Loss (MSE): 1.1862, Time: 5.20s\n",
      "Epoch [16/20], Validation Loss (MSE): 1.2715, Validation RMSE: 1.1276, Validation Accuracy (Rounded): 33.31%\n",
      "--------------------------------------------------\n",
      "Epoch [17/20], Training Loss (MSE): 1.1857, Time: 5.15s\n",
      "Epoch [17/20], Validation Loss (MSE): 1.2694, Validation RMSE: 1.1267, Validation Accuracy (Rounded): 33.70%\n",
      "--------------------------------------------------\n",
      "Epoch [18/20], Training Loss (MSE): 1.1843, Time: 5.21s\n",
      "Epoch [18/20], Validation Loss (MSE): 1.2705, Validation RMSE: 1.1271, Validation Accuracy (Rounded): 33.19%\n",
      "--------------------------------------------------\n",
      "Epoch [19/20], Training Loss (MSE): 1.1836, Time: 5.38s\n",
      "Epoch [19/20], Validation Loss (MSE): 1.2741, Validation RMSE: 1.1288, Validation Accuracy (Rounded): 33.24%\n",
      "--------------------------------------------------\n",
      "Epoch [20/20], Training Loss (MSE): 1.1827, Time: 5.30s\n",
      "Epoch [20/20], Validation Loss (MSE): 1.2702, Validation RMSE: 1.1270, Validation Accuracy (Rounded): 33.66%\n",
      "--------------------------------------------------\n",
      "Training complete.\n",
      "\n",
      "Evaluating on the Test Set...\n",
      "\n",
      "FINAL TEST SET PERFORMANCE:\n",
      "Test Loss (MSE): 1.2751\n",
      "Test RMSE: 1.1292\n",
      "Test Accuracy (Rounded): 33.93%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20 # Adjust as needed\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "for epoch in range(num_epochs): # Standard Python range: 0 to num_epochs-1\n",
    "    model.train() # Set model to training mode\n",
    "    total_train_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    epoch_end_time = time.time()\n",
    "    # Use epoch + 1 for 1-based indexing in print statements\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss (MSE): {avg_train_loss:.4f}, Time: {epoch_end_time - epoch_start_time:.2f}s')\n",
    "\n",
    "    # Validation phase (evaluate on validation set after each epoch)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_val_loss_mse = 0\n",
    "    correct_val_predictions = 0\n",
    "    total_val_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader: # Use val_loader here\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions = model(features) # Model outputs continuous values (0-5)\n",
    "            val_loss = criterion(predictions, labels)\n",
    "            total_val_loss_mse += val_loss.item()\n",
    "\n",
    "            # Accuracy calculation for validation set\n",
    "            rounded_predictions = torch.round(predictions)\n",
    "            correct_val_predictions += (rounded_predictions == labels).sum().item()\n",
    "            total_val_samples += labels.size(0)\n",
    "\n",
    "    avg_val_loss_mse = total_val_loss_mse / len(val_loader)\n",
    "    avg_val_loss_rmse = np.sqrt(avg_val_loss_mse)\n",
    "    val_accuracy = (correct_val_predictions / total_val_samples) * 100 if total_val_samples > 0 else 0\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss (MSE): {avg_val_loss_mse:.4f}, Validation RMSE: {avg_val_loss_rmse:.4f}, Validation Accuracy (Rounded): {val_accuracy:.2f}%')\n",
    "    print(\"-\" * 50) # Increased separator length for clarity\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Final Evaluation on the Test Set ---\n",
    "# This should be done only once, after all training and validation is complete.\n",
    "print(\"\\nEvaluating on the Test Set...\")\n",
    "model.eval() # Ensure model is in evaluation mode\n",
    "total_test_loss_mse = 0\n",
    "correct_test_predictions = 0\n",
    "total_test_samples = 0\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader: # Use test_loader here\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        predictions = model(features) # Model outputs continuous values (0-5)\n",
    "        test_loss = criterion(predictions, labels)\n",
    "        total_test_loss_mse += test_loss.item()\n",
    "\n",
    "        # Accuracy calculation for test set\n",
    "        rounded_predictions = torch.round(predictions)\n",
    "        correct_test_predictions += (rounded_predictions == labels).sum().item()\n",
    "        total_test_samples += labels.size(0)\n",
    "\n",
    "avg_test_loss_mse = total_test_loss_mse / len(test_loader)\n",
    "avg_test_loss_rmse = np.sqrt(avg_test_loss_mse)\n",
    "test_accuracy = (correct_test_predictions / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "\n",
    "print(f'\\nFINAL TEST SET PERFORMANCE:')\n",
    "print(f'Test Loss (MSE): {avg_test_loss_mse:.4f}')\n",
    "print(f'Test RMSE: {avg_test_loss_rmse:.4f}')\n",
    "print(f'Test Accuracy (Rounded): {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
