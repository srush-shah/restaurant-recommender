{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc88960-3fcf-46d2-967d-ab7209c6a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b054c352-d87e-42b1-9e53-b0834c99d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchinfo import summary\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9583a334-edb1-4f79-9d0a-5f116a208de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_feature_dim, hidden_dim_1, hidden_dim_2, output_size=1): \n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_feature_dim, hidden_dim_1)\n",
    "        self.relu1 = nn.ReLU()                        \n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_logits = nn.Linear(hidden_dim_2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        logits = self.output_logits(x)\n",
    "        \n",
    "        # Scale sigmoid output to be between 0 and 5\n",
    "        rating = 5.0 * torch.sigmoid(logits)\n",
    "        return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99c84686-bbc2-4550-9c8b-1fda59969876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2847/4201624881.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleMLP                                --\n",
       "├─Linear: 1-1                            108,288\n",
       "├─ReLU: 1-2                              --\n",
       "├─Linear: 1-3                            32,896\n",
       "├─ReLU: 1-4                              --\n",
       "├─Linear: 1-5                            129\n",
       "=================================================================\n",
       "Total params: 141,313\n",
       "Trainable params: 141,313\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = 422\n",
    "H1, H2 = 256, 128\n",
    "\n",
    "model_path = \"./models/simple_mlp_ratings_statedict.pth\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = SimpleMLP(INPUT_DIM, H1, H2).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cecc31-8f94-4709-adff-61df675a2d1c",
   "metadata": {},
   "source": [
    "# Pytorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea7c3f-370c-418a-9fcd-8e8508cea267",
   "metadata": {},
   "source": [
    "### Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c30e4d5c-8c21-4c07-bb5a-5af90a8d190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 0.57 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a2057d-c966-44fe-8889-69f237041284",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv(\"DATA_DIR\", \"/mnt/object\")\n",
    "df_test = pd.read_csv(data_dir+\"/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec7c0877-2786-44cf-8152-32f745c53932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (139332, 422), y_test shape: (139332,)\n"
     ]
    }
   ],
   "source": [
    "all_cols = df_test.columns.tolist()\n",
    "col_exclude = ['stars', 'Unnamed: 0']\n",
    "model_feature_columns = [col for col in all_cols if col not in col_exclude]\n",
    "\n",
    "X_test = df_test[model_feature_columns]\n",
    "y_test = df_test['stars']  \n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95c6631e-b076-4ed2-a12b-9efb5934978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3536eb7c-418c-45ff-8b7e-35be29483b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048 # Adjust based on V100 VRAM and dataset size\n",
    "num_data_workers = 4 # Good starting point for V100\n",
    "\n",
    "test_loader = DataLoader( # Test loader\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_data_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db9c97-2292-4e8f-abf1-0efb4b92690e",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdf88dc0-d177-41ef-a203-4b89fb5f60f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TEST SET PERFORMANCE:\n",
      "Test Loss (MSE): 1.2751\n",
      "Test RMSE: 1.1292\n",
      "Test Accuracy (Rounded): 33.93%\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Ensure model is in evaluation mode\n",
    "total_test_loss_mse = 0\n",
    "correct_test_predictions = 0\n",
    "total_test_samples = 0\n",
    "criterion = nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader: # Use test_loader here\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        predictions = model(features) # Model outputs continuous values (0-5)\n",
    "        test_loss = criterion(predictions, labels)\n",
    "        total_test_loss_mse += test_loss.item()\n",
    "\n",
    "        # Accuracy calculation for test set\n",
    "        rounded_predictions = torch.round(predictions)\n",
    "        correct_test_predictions += (rounded_predictions == labels).sum().item()\n",
    "        total_test_samples += labels.size(0)\n",
    "\n",
    "avg_test_loss_mse = total_test_loss_mse / len(test_loader)\n",
    "avg_test_loss_rmse = np.sqrt(avg_test_loss_mse)\n",
    "test_accuracy = (correct_test_predictions / total_test_samples) * 100 if total_test_samples > 0 else 0\n",
    "\n",
    "print(f'\\nFINAL TEST SET PERFORMANCE:')\n",
    "print(f'Test Loss (MSE): {avg_test_loss_mse:.4f}')\n",
    "print(f'Test RMSE: {avg_test_loss_rmse:.4f}')\n",
    "print(f'Test Accuracy (Rounded): {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6510ebc-812f-4109-867f-9a926310fbed",
   "metadata": {},
   "source": [
    "### Inference Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7240131-4ced-4a49-9b66-b1b1b85ef812",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 100  # Number of trials\n",
    "\n",
    "# Get a single sample from the test data\n",
    "\n",
    "single_sample, _ = next(iter(test_loader))  \n",
    "single_sample = single_sample[0].unsqueeze(0)  \n",
    "\n",
    "# Warm-up run \n",
    "with torch.no_grad():\n",
    "    model(single_sample)\n",
    "\n",
    "latencies = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        _ = model(single_sample)\n",
    "        latencies.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1afd97a6-3b9b-4ac0-be89-31644bb3273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency (single sample, median): 0.15 ms\n",
      "Inference Latency (single sample, 95th percentile): 0.22 ms\n",
      "Inference Latency (single sample, 99th percentile): 0.28 ms\n",
      "Inference Throughput (single sample): 6339.64 FPS\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479d000-b12c-4322-a5e1-88b5bfa22aa3",
   "metadata": {},
   "source": [
    "### Batch Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8c2c058-6c52-4495-bcb0-42611df85b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 50  # Number of trials\n",
    "\n",
    "# Get a batch from the test data\n",
    "batch_input, _ = next(iter(test_loader))  \n",
    "\n",
    "# Warm-up run \n",
    "with torch.no_grad():\n",
    "    model(batch_input)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        _ = model(batch_input)\n",
    "        batch_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d623519-f3da-4f5e-91a5-d3d74cce5769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Throughput: 794569.74 FPS\n"
     ]
    }
   ],
   "source": [
    "batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048751b-611b-4c04-a7b7-64dbe862363a",
   "metadata": {},
   "source": [
    "### Summary Of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9df2a417-69aa-4cbb-8e88-1b17b691b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 0.57 MB\n",
      "Accuracy: 33.93% (47280/139332 correct)\n",
      "Inference Latency (single sample, median): 0.15 ms\n",
      "Inference Latency (single sample, 95th percentile): 0.22 ms\n",
      "Inference Latency (single sample, 99th percentile): 0.28 ms\n",
      "Inference Throughput (single sample): 6339.64 FPS\n",
      "Batch Throughput: 794569.74 FPS\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
    "print(f\"Accuracy: {test_accuracy:.2f}% ({correct_test_predictions}/{total_test_samples} correct)\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2cbb7a-8f5e-46f4-be61-5d76758c11b9",
   "metadata": {},
   "source": [
    "# ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416b1230-4e35-4b1c-98cb-d704bf902e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccdd858d-8bbd-4c3b-af7d-a1f4875ca7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved to ./models/simple_mlp_ratings_statedict.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"./models/simple_mlp_ratings_statedict.onnx\"\n",
    "\n",
    "dummy = torch.randn(1, INPUT_DIM)\n",
    "torch.onnx.export(\n",
    "    model, dummy, onnx_model_path,\n",
    "    input_names=[\"features\"],\n",
    "    output_names=[\"rating\"],\n",
    "    opset_version=13, dynamic_axes={\"features\": {0: \"batch_size\"}, \"rating\": {0: \"batch_size\"}}\n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93fd469c-c893-49ff-a9b6-559aaa12f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c730521-26cb-4a4a-b9b0-ac95eb50d5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPUExecutionProvider']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "ort_session.get_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d4525-dd8e-48bd-864f-5535f54d0170",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abbcfbc7-dd00-4679-a889-e9e778d546a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TEST SET PERFORMANCE:\n",
      "Test Loss (MSE): 1.2719\n",
      "Test RMSE: 1.1278\n",
      "Test Accuracy (Rounded): 33.93%\n"
     ]
    }
   ],
   "source": [
    "total_sq_err = 0.0\n",
    "total_samples = 0\n",
    "correct_rounded = 0\n",
    "\n",
    "for features, labels in test_loader:\n",
    "    # 1) Move to CPU‐numpy\n",
    "    feats_np  = features.cpu().numpy().astype(np.float32)\n",
    "    labels_np = labels.cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # 2) ONNX inference\n",
    "    preds_np = ort_session.run(None, {\"features\": feats_np})[0].squeeze(1)  # shape (batch,)\n",
    "    labels_np = labels_np.reshape(-1)\n",
    "    \n",
    "    \n",
    "    # 3) Accumulate squared errors\n",
    "    errs = preds_np - labels_np\n",
    "    total_sq_err    += np.sum(errs ** 2)\n",
    "    total_samples   += labels_np.shape[0]\n",
    "    \n",
    "    # 4) “Accuracy” by rounding to nearest star\n",
    "    rounded = np.rint(preds_np)\n",
    "    correct_rounded += np.sum(rounded == labels_np)\n",
    "\n",
    "# 5) Final metrics\n",
    "mse  = total_sq_err / total_samples\n",
    "rmse = np.sqrt(mse)\n",
    "acc  = 100.0 * correct_rounded / total_samples\n",
    "\n",
    "print(f'\\nFINAL TEST SET PERFORMANCE:')\n",
    "print(f'Test Loss (MSE): {mse:.4f}')\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "print(f'Test Accuracy (Rounded): {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b484361-cc7b-4534-837c-f394c321b1a2",
   "metadata": {},
   "source": [
    "### Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d61e5ec1-3543-48d0-806e-9fd333b40698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 0.57 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a87635-51a1-42fe-8953-bf14f708167a",
   "metadata": {},
   "source": [
    "### Inference Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43549da4-65ab-4b5a-af47-ba82606a74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 100  # Number of trials\n",
    "\n",
    "# Get a single sample from the test data\n",
    "\n",
    "single_sample, _ = next(iter(test_loader))  \n",
    "single_sample = single_sample[:1].numpy()\n",
    "\n",
    "# Warm-up run\n",
    "ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "\n",
    "latencies = []\n",
    "for _ in range(num_trials):\n",
    "    start_time = time.time()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "    latencies.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9f703d1-c652-4bc5-b2a6-fa3e61c66d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency (single sample, median): 0.05 ms\n",
      "Inference Latency (single sample, 95th percentile): 0.08 ms\n",
      "Inference Latency (single sample, 99th percentile): 0.15 ms\n",
      "Inference Throughput (single sample): 16532.53 FPS\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b30d5a-dcef-496d-a1fa-1b9bd01d29e2",
   "metadata": {},
   "source": [
    "### Batch Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f8fe032-241b-497c-a804-820975c334cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 50  # Number of trials\n",
    "\n",
    "# Get a batch from the test data\n",
    "batch_input, _ = next(iter(test_loader))  \n",
    "batch_input = batch_input.numpy()\n",
    "\n",
    "# Warm-up run\n",
    "ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "\n",
    "batch_times = []\n",
    "for _ in range(num_batches):\n",
    "    start_time = time.time()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "    batch_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b7d22e7-db70-4b0b-956b-b6be61a7d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Throughput: 784385.82 FPS\n"
     ]
    }
   ],
   "source": [
    "batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a0977d6-b802-40ef-8ab9-eb24893f8ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.93% (47280/139332 correct)\n",
      "Model Size on Disk: 0.57 MB\n",
      "Inference Latency (single sample, median): 0.05 ms\n",
      "Inference Latency (single sample, 95th percentile): 0.08 ms\n",
      "Inference Latency (single sample, 99th percentile): 0.15 ms\n",
      "Inference Throughput (single sample): 16532.53 FPS\n",
      "Batch Throughput: 784385.82 FPS\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc:.2f}% ({correct_rounded}/{total_samples} correct)\")\n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fb7b4-9d5e-4e8f-ae45-8302c5f0b124",
   "metadata": {},
   "source": [
    "# Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6bb7b73-57f6-4616-834f-8926ced36a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    ## Benchmark accuracy\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in test_loader:\n",
    "        features  = features.cpu().numpy().astype(np.float32)\n",
    "        labels = labels.cpu().numpy().astype(np.float32)\n",
    "        predicted = ort_session.run(None, {\"features\": features})[0].squeeze(1)\n",
    "        labels = labels.reshape(-1)\n",
    "        total += labels.shape[0]\n",
    "        rounded = np.rint(predicted)\n",
    "        correct += np.sum(rounded == labels)\n",
    "    accuracy = (correct / total) * 100\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "\n",
    "    ## Benchmark inference latency for single sample\n",
    "\n",
    "    num_trials = 100  # Number of trials\n",
    "\n",
    "    # Get a single sample from the test data\n",
    "\n",
    "    single_sample, _ = next(iter(test_loader))  \n",
    "    single_sample = single_sample[:1].numpy()\n",
    "\n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start_time)\n",
    "\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    ## Benchmark batch throughput\n",
    "\n",
    "    num_batches = 50  # Number of trials\n",
    "\n",
    "    # Get a batch from the test data\n",
    "    batch_input, _ = next(iter(test_loader))  \n",
    "    batch_input = batch_input.numpy()\n",
    "\n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start_time)\n",
    "\n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556347d-e98f-4171-af1f-3782a34557f3",
   "metadata": {},
   "source": [
    "### Basic graph optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "72f80174-7161-4d3a-9e9e-9af0a733af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_path = \"models/simple_mlp_optimized.onnx\"\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED # apply graph optimizations\n",
    "session_options.optimized_model_filepath = optimized_model_path \n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, sess_options=session_options, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "197cef34-e452-4236-9028-87db393bcaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution provider: ['CPUExecutionProvider']\n",
      "Accuracy: 33.93% (47280/139332 correct)\n",
      "Inference Latency (single sample, median): 0.05 ms\n",
      "Inference Latency (single sample, 95th percentile): 0.07 ms\n",
      "Inference Latency (single sample, 99th percentile): 0.10 ms\n",
      "Inference Throughput (single sample): 20344.90 FPS\n",
      "Batch Throughput: 457573.44 FPS\n"
     ]
    }
   ],
   "source": [
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3a6a3-7274-425a-83bc-f1a91f67481e",
   "metadata": {},
   "source": [
    "### Dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6c3ce67d-f47e-442a-b34c-87a4ffa863e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping neural_compressor as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y neural_compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dae01f39-96d4-4bf5-96dc-0955ee87978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "45deaceb-3708-4250-9ec7-eb7f14c55bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in /opt/conda/lib/python3.12/site-packages (1.22.0)\n",
      "Requirement already satisfied: onnxruntime-tools in /opt/conda/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (2.2.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (24.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (5.28.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.12/site-packages (from onnxruntime-tools) (1.17.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from onnxruntime-tools) (6.1.1)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.12/site-packages (from onnxruntime-tools) (9.0.0)\n",
      "Requirement already satisfied: py3nvml in /opt/conda/lib/python3.12/site-packages (from onnxruntime-tools) (0.2.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.12/site-packages (from py3nvml->onnxruntime-tools) (0.14.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime onnxruntime-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "44a38742-486f-4740-b02a-2bd8f5598223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4cac2111-bdaf-4f3b-b993-2a0ab555431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model written to: models/simple_mlp_quantized_dynamic.onnx\n"
     ]
    }
   ],
   "source": [
    "quant_model =  \"models/simple_mlp_quantized_dynamic.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=onnx_model_path,\n",
    "    model_output=quant_model,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    per_channel=True    # set True to apply ONNX graph optimizations before quant\n",
    ")\n",
    "\n",
    "print(f\"Quantized model written to: {quant_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ad4e1905-34ec-4f4c-88f0-bdef0dbc7418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 0.15 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(quant_model) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b5cd6ffb-b467-4ab4-9860-0edcec60555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution provider: ['CPUExecutionProvider']\n",
      "Accuracy: 33.92% (47263/139332 correct)\n",
      "Inference Latency (single sample, median): 0.07 ms\n",
      "Inference Latency (single sample, 95th percentile): 0.10 ms\n",
      "Inference Latency (single sample, 99th percentile): 0.13 ms\n",
      "Inference Throughput (single sample): 13161.90 FPS\n",
      "Batch Throughput: 764079.50 FPS\n"
     ]
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession(quant_model, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17f0aa-cb96-42cf-820b-4b252c958d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
